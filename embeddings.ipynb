{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "27c3f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would ha\n",
      "\n",
      "Chars: 20479\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Cargar texto\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "raw_text = raw_text.replace(\"\\n\", \" \").strip()\n",
    "print(raw_text[:300])\n",
    "print(\"\\nChars:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4c665e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens (simple regex): 4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"N tokens (simple regex):\", len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b09bbd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1132\n",
      "IDs especiales: 1130 1131\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# Tokens especiales (clave)\n",
    "special_tokens = [\"<|unk|>\", \"<|endoftext|>\"]\n",
    "for tok in special_tokens:\n",
    "    if tok not in vocab:\n",
    "        vocab[tok] = len(vocab)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"IDs especiales:\", vocab[\"<|unk|>\"], vocab[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a4d81344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1130, 5, 355, 1126, 628, 975, 7, 1130, 999, 6, 115, 1130, 10]\n",
      "Decoded: <|unk|>, do you like tea. <|unk|> this -- a <|unk|>?\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[\"<|unk|>\"]) for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer_simple = SimpleTokenizerV1(vocab)\n",
    "\n",
    "test_text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "encoded = tokenizer_simple.encode(test_text)\n",
    "decoded = tokenizer_simple.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded[:20])\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f69455be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Need at least max_length+1 tokens\"\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e0af3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "\n",
      "Shapes: torch.Size([8, 4]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "inputs, targets = next(iter(dataloader))\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "print(\"\\nShapes:\", inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6fe72b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length=16, stride=16 -> muestras = 316\n",
      "max_length=16, stride=8 -> muestras = 631\n",
      "max_length=32, stride=8 -> muestras = 629\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def n_samples(txt, max_length, stride):\n",
    "    ds = GPTDatasetV1(txt, tokenizer, max_length=max_length, stride=stride)\n",
    "    return len(ds)\n",
    "\n",
    "configs = [\n",
    "    (16, 16),  # sin superposición\n",
    "    (16, 8),   # 50% overlap\n",
    "    (32, 8),   # más contexto + overlap alto\n",
    "]\n",
    "\n",
    "for ml, st in configs:\n",
    "    print(f\"max_length={ml}, stride={st} -> muestras = {n_samples(raw_text, ml, st)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d41cb2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: torch.Size([8, 4])\n",
      "token_embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257      # GPT-2 vocab\n",
    "embed_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Tomar un batch pequeño\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "inputs, targets = next(iter(dataloader))\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"inputs shape:\", inputs.shape)\n",
    "print(\"token_embeddings shape:\", token_embeddings.shape)  # (batch, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1bab590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions: tensor([0, 1, 2, 3])\n",
      "pos_embeddings shape: torch.Size([4, 256])\n",
      "x shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, embed_dim)\n",
    "\n",
    "positions = torch.arange(context_length)\n",
    "pos_embeddings = pos_embedding_layer(positions)\n",
    "\n",
    "print(\"positions:\", positions)\n",
    "print(\"pos_embeddings shape:\", pos_embeddings.shape)  # (seq_len, embed_dim)\n",
    "\n",
    "# Combinar token + posición\n",
    "x = token_embeddings + pos_embeddings\n",
    "print(\"x shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b4c46",
   "metadata": {},
   "source": [
    "# Tokenización: pasar de texto a unidades que podamos manejar\n",
    "\n",
    "La tokenización es el primer paso para poder trabajar con lenguaje natural en un modelo. Básicamente nos convierte el texto en pequeñas unidades llamadas tokens, que luego se van a transforman en números (IDs). Esto es importante porque las redes neuronales no entienden palabras directamente, sino que entienden números.\n",
    "\n",
    "Sin tokenización no podríamos entrenar un modelo de lenguaje, ya que no habría una forma estructurada de representar el texto. En el contexto de agentes, es importante: que el agente necesita representar instrucciones, contexto, memoria y respuestas de forma consistente para poder razonar y generar acciones que tengan sentido. En pocas palabras, tokenizar es convertir lenguaje humano en algo que una red neuronal pueda procesar matemáticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c1b0b",
   "metadata": {},
   "source": [
    "# Vocabulario + <|unk|> y robustez\n",
    "\n",
    "Cuando estamos construyendo un vocabulario, asignamos un ID único a cada token la cual aparece en el corpus. Esto permite que cada palabra o subpalabra tenga una representación numérica asignada fija dentro del modelo, aunque en el mundo real siempre aparecen palabras nuevas: nombres propios, errores de escritura, términos técnicos, entre otros.\n",
    "\n",
    "Aquí es donde vemos el uso de el token <|unk|>. Este token funciona como respaldo cuando aparece algo que no está en el vocabulario. En vez de que el sistema falle, el modelo asigna ese token desconocido y sigue funcionando. Esto aporta robustez y confianza al pipeline.\n",
    "\n",
    "En sistemas de agentes esto es fundamental, porque los agentes reciben entradas impredecibles (usuarios, herramientas externas, logs, datos dinámicos). Si no existiera un mecanismo como <|unk|>, el sistema podría romperse fácilmente ante cualquier palabra nueva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9553d3",
   "metadata": {},
   "source": [
    "# Ventanas (max_length/stride) y overlap\n",
    "\n",
    "Los modelos de lenguaje se entrenan para predecir el siguiente token. Para poder lograrlo, el texto completo se divide en secuencias de longitud fija max_length, luego se recorren usando un paso stride. \n",
    "\n",
    "Si el stride es menor que max_length, se va a genera superposición entre las ventanas. Esto va a significar que algunos tokens aparecen en múltiples secuencias de entrenamiento, pero en contextos la cual son muy poco distintos. Esta técnica ayuda al modelo a aprender mejor las dependencias locales y reduce los huecos entre fragmentos de texto.\n",
    "\n",
    "en pocas palabras, el overlap funciona como una forma de aumentar los datos sin necesidad de recolectar más texto. Le da al modelo más oportunidades de ver patrones similares en distintos contextos, lo cual mejora su capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d028554",
   "metadata": {},
   "source": [
    "# ¿Por qué los embeddings codifican significado? Relación con redes neuronales\n",
    "\n",
    "Los embeddings lo que hacen es que codifican significado porque son vectores que se aprenden durante el entrenamiento del modelo. No son representaciones aleatorias, se estan optimizando, usando gradiente para resolver una tarea, como predecir el siguiente token. \n",
    "\n",
    "Cuando dos palabras aparecen en contextos similares, el modelo ajusta sus vectores para que produzcan activaciones parecidas en la red. Como resultado, esos vectores terminan ubicándose cerca en el espacio vectorial.\n",
    "\n",
    "Desde el punto de vista de las redes neuronales, el embedding es una capa con pesos entrenables (una tabla de vectores) que transforma IDs discretos en representaciones continuas. Estas representaciones luego pasan por más capas que capturan patrones más complejos y relaciones de largo alcance.\n",
    "\n",
    "En sistemas de agentes, los embeddings son esenciales porque lo que hacen es permitir comparar información, hacer búsqueda semántica, recuperar memoria y medir similitud entre conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f01f2c",
   "metadata": {},
   "source": [
    "## Resultado del experimento (max_length/stride)\n",
    "\n",
    "Al bajar el stride aumentan las muestras porque se generan más ventanas superpuestas del mismo texto. Eso hace que el modelo vea los mismos tokens en distintos contextos, lo cual nos ayuda a que pueda aprender mejor dependencias locales y a no perder transiciones entre ventanas. Por eso el overlap funciona como una forma de aumentar datos sin traer más texto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
